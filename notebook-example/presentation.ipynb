{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14a2a4d-4400-4786-be8a-de9a85cc0b1d",
   "metadata": {},
   "source": [
    "# Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d25b0-00a7-457c-b0df-14496994078e",
   "metadata": {},
   "source": [
    "## What are Notebooks?\n",
    "\n",
    "Jupyter Notebooks are interactive, web-based tools that allow developers and data scientists to combine code, visualizations, and narrative text in a single document. \n",
    "\n",
    "They are ideal for data analysis, experimentation, and educational purposes. The notebook format enables users to write and execute code in cells, view outputs immediately, and create a narrative around their work, making it accessible and understandable for both technical and non-technical audiences. This makes Jupyter Notebooks a powerful platform for collaboration, teaching, and sharing insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db97ceb-0828-4477-8b3c-3bf9e37b1171",
   "metadata": {},
   "source": [
    "Useful for:\n",
    "- EDA (exploratory data analysis)\n",
    "- creating AI model POCs\n",
    "- anything where you want visibility of every step of the process or want to build up and adjust a process without needing to rerun the whole flow from the start\n",
    "\n",
    "Less useful for:\n",
    "- creating production ready code\n",
    "- working on code that a number of people will want to be editing at the same time\n",
    "\n",
    "So within these bounds, lets look at various ways we can make our jupyter lives easier and more collaborative when working in a team environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b0571-3859-4ab5-91b9-d9b4f50242b9",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Never a developers favourite thing. Data scientists are marginally better but can often make a lot of assumptions about people's knowledge of the domain problem being explored.\n",
    "\n",
    "Note: I am a huge documentation fan, don't come at me with your \"self-documenting code\" arguments.\n",
    "\n",
    "Recommendations:\n",
    "- add a documentation block at the top of the notebook that describes the aims and outcomes of the notebook\n",
    "- if you're building out a process or model, document the unspecified dependencies (data requirements, model lifecycle requirements etc) at the top. Include any:\n",
    "  - gotchas\n",
    "  - assumptions\n",
    "  - work still to complete\n",
    "- link to any relevent external documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d8c73-1b86-4f12-a69e-dba54af9ce0d",
   "metadata": {},
   "source": [
    "## Structuring the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e19ef-aec4-4ba4-ae0e-85f49d2cbe32",
   "metadata": {},
   "source": [
    "### Navigating the Notebook\n",
    "- break the notebook into sections\n",
    "  - put setup and config at the top, so it's obvious what the dependencies are and what the main setup is\n",
    "  - use logical sections with markdown headings\n",
    "- collapse sections to reduce scroll / eye overwhelm\n",
    "- magic methods can help to show method content from elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a02ea1-1090-4f34-8a36-fc0a7ca5bde5",
   "metadata": {},
   "source": [
    "### External Modules\n",
    "These are especially useful where notebooks in different projects utilise the same methods, e.g. for reading from a database or applying standardised data manipulations.\n",
    "\n",
    "- encourages code reuse, so each notebook isn't reinventing the wheel in slightly different ways\n",
    "- notebooks can use the same modules as production code, so you can be sure for those methods that the research and production code are in sync\n",
    "- tidies the main flow, so that the notebook contains the code specific to this particular model or analysis\n",
    "- if you are importing modules that you are changing whilst running the notebook, you can use autoreloading to update the notebook as soon as that external module as been resaved rather than having to restart everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9241813-4c95-4304-b7df-6e4cd08a2cc6",
   "metadata": {},
   "source": [
    "## Gotchas\n",
    "- state\n",
    "- easy to accidentally save secrets or config only applicable to you\n",
    "- saving and creating checkpoints. It can be easy to accidentally close a notebook without hitting save, in which instance you will lose the latest changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb344503-8924-4ff5-a483-c982a715becb",
   "metadata": {},
   "source": [
    "## Code Reviews\n",
    "- `.ipynb` are essentially JSON files, which makes them _very_ hard to check in a pull/merge request\n",
    "- for notebooks that need to follow a peer review process for the coding elements, use an extension such as JupytText to be able to parse the changesets much more easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dad628-faa6-4e64-8ba0-37adca240b33",
   "metadata": {},
   "source": [
    "## Saving Cell Output\n",
    "Sometimes you want to save your notebooks with all the outputs in place, so that they are easily shareable and tell the story that you want. But if the notebooks are going into source control and contain references to customer data, this is often not desired.\n",
    "\n",
    "This is another thing that Jupytext can help with. You can configure you .gitignore to exclude the `.ipynb` files and just include the `.py` files, that don't include the output.\n",
    "\n",
    "Other options are Git pre-commit hooks to strip the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dbffe-c56c-4cb5-a377-beeda39d8985",
   "metadata": {},
   "source": [
    "## Getting Code to Production\n",
    "Things you'll need to think about when porting code to a live production system:\n",
    "- notebooks usually run off static CSVs, for easily repeatable experiments. A live production system will be running off data that is constantly updating, so will require a lot of data validation checks to ensure that there is enough workable data to actually run the model\n",
    "- production systems do not allow you to eyeball the output of a process as it goes along, so you'll need additional validation around the outputs of the code, such as model accuracy tracking and sanity checks around data, if its meant to be within expected ranges for example\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
